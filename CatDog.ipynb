{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building a model\n",
    "from keras.models import Sequential                       # building a sequential model\n",
    "from keras.layers import Conv2D                           # convolution layer\n",
    "from keras.layers import MaxPooling2D                     # pooling layer\n",
    "from keras.layers import Dropout                          # regularization, prevent overfitting\n",
    "from keras.layers import Dense                            # fully connected layers\n",
    "from keras.layers import GlobalMaxPooling2D               # flattening for fully connected layers\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "# dealing with images\n",
    "from keras.preprocessing.image import ImageDataGenerator  # data augmentation\n",
    "from keras.utils import to_categorical                    # classification\n",
    "from sklearn.model_selection import train_test_split      # split training into train/test\n",
    "\n",
    "# utilities\n",
    "import random                                             # seed for train_test_split\n",
    "import numpy as np                                        # linear algebra, of course\n",
    "import pandas as pd                                       # reading csv and storing image data\n",
    "import os                                                 # listing files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['train', 'sampleSubmission.csv', 'test1.zip']\n",
      "25000 training images\n"
     ]
    }
   ],
   "source": [
    "data_dir = '../../catdog_input'                           # set the base data directory\n",
    "training_dir = data_dir+'/train'                          # set the training directory\n",
    "print(os.listdir(data_dir))                               # see which files are in the data directory\n",
    "\n",
    "nb_train_imgs = len(os.listdir(training_dir))             # get the number of training images\n",
    "print(nb_train_imgs, 'training images')                   # see how many training images there are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "imgs = os.listdir(training_dir)                           # list of image file names\n",
    "labels = np.zeros((nb_train_imgs,), dtype='object')       # init. empty array for labels\n",
    "for i, img in enumerate(imgs):                            # iterate over each image\n",
    "    label = img.split('.')[0]                             # extract the class from file name\n",
    "    if label == 'dog':                                    # if the label is dog, insert 1 to label array\n",
    "        labels[i] = '1'\n",
    "    else:                                                 # if the label is cat, insert 0 to label array\n",
    "        labels[i] = '0'\n",
    "        \n",
    "df = pd.DataFrame({                                       # create dataframe to feed to keras\n",
    "    'filename': imgs,                                     # image file names\n",
    "    'category': labels                                    # class labels\n",
    "})\n",
    "\n",
    "for i in range(10):                                       # iterate over a few samples\n",
    "    print(labels[i])                                      # print label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24995</th>\n",
       "      <td>0</td>\n",
       "      <td>cat.4228.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24996</th>\n",
       "      <td>1</td>\n",
       "      <td>dog.2454.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24997</th>\n",
       "      <td>1</td>\n",
       "      <td>dog.8236.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24998</th>\n",
       "      <td>1</td>\n",
       "      <td>dog.2249.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24999</th>\n",
       "      <td>1</td>\n",
       "      <td>dog.11217.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      category       filename\n",
       "24995        0   cat.4228.jpg\n",
       "24996        1   dog.2454.jpg\n",
       "24997        1   dog.8236.jpg\n",
       "24998        1   dog.2249.jpg\n",
       "24999        1  dog.11217.jpg"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()  # ensure label matches filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_281 (Conv2D)          (None, 128, 128, 64)      1792      \n",
      "_________________________________________________________________\n",
      "conv2d_282 (Conv2D)          (None, 128, 128, 64)      36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_143 (MaxPoolin (None, 64, 64, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_283 (Conv2D)          (None, 64, 64, 128)       73856     \n",
      "_________________________________________________________________\n",
      "conv2d_284 (Conv2D)          (None, 64, 64, 128)       147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_144 (MaxPoolin (None, 32, 32, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_285 (Conv2D)          (None, 32, 32, 256)       295168    \n",
      "_________________________________________________________________\n",
      "conv2d_286 (Conv2D)          (None, 32, 32, 256)       590080    \n",
      "_________________________________________________________________\n",
      "conv2d_287 (Conv2D)          (None, 32, 32, 256)       590080    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_145 (MaxPoolin (None, 16, 16, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_288 (Conv2D)          (None, 16, 16, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "conv2d_289 (Conv2D)          (None, 16, 16, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "conv2d_290 (Conv2D)          (None, 16, 16, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_146 (MaxPoolin (None, 8, 8, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_291 (Conv2D)          (None, 8, 8, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "conv2d_292 (Conv2D)          (None, 8, 8, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "conv2d_293 (Conv2D)          (None, 8, 8, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_147 (MaxPoolin (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "global_max_pooling2d_9 (Glob (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_77 (Dense)             (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout_113 (Dropout)        (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_78 (Dense)             (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 14,977,857\n",
      "Trainable params: 14,977,857\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "img_wh = 128                           # image width and height\n",
    "img_d = 3                              # number of channels, RGB is 3 channels\n",
    "input_shape = (img_wh, img_wh, img_d)  # create shape for width, height, and channels\n",
    "\n",
    "# a note on how padding='same' is calculated. here we use default strides of 1\n",
    "#   out_height = ceil(float(img_wh) / float(strides[1]))\n",
    "#   out_width  = ceil(float(img_wh)) / float(strides[2]))\n",
    "\n",
    "# build a VGG style sequential model\n",
    "model = Sequential([\n",
    "    # block 1\n",
    "    # 2 convolutions, each with:\n",
    "    #    64 3x3 kernels,\n",
    "    #    same padding (as described above),\n",
    "    #    relu activation\n",
    "    # and a maxpooling layer with strides of 2 and a window size of 2x2\n",
    "    Conv2D(64, (3, 3), input_shape=input_shape, padding='same', activation='relu'), \n",
    "    Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "    MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),\n",
    "    \n",
    "    # block 2\n",
    "    # 2 convolutions, each with:\n",
    "    #   128 3x3 kernels,\n",
    "    #   same padding (as described above),\n",
    "    #   relu activation\n",
    "    # and a maxpooling layer with strides of 2 and a window size of 2x2\n",
    "    Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "    Conv2D(128, (3, 3), activation='relu', padding='same',),\n",
    "    MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),\n",
    "    \n",
    "    # block 3\n",
    "    # 3 convolutions, each with: \n",
    "    #   256 3x3 kernels,\n",
    "    #   same padding (as described above),\n",
    "    #   relu activation\n",
    "    # and a maxpooling layer with strides of 2 and a window size of 2x2\n",
    "    Conv2D(256, (3, 3), activation='relu', padding='same',),\n",
    "    Conv2D(256, (3, 3), activation='relu', padding='same',),\n",
    "    Conv2D(256, (3, 3), activation='relu', padding='same',),\n",
    "    MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),\n",
    "    \n",
    "    # block 4\n",
    "    # 3 convolutions, each with:\n",
    "    #   512 3x3 kernels,\n",
    "    #   same padding (as described above),\n",
    "    #   relu activation\n",
    "    # and a maxpooling layer with strides of 2 and a window size of 2x2\n",
    "    Conv2D(512, (3, 3), activation='relu', padding='same',),\n",
    "    Conv2D(512, (3, 3), activation='relu', padding='same',),\n",
    "    Conv2D(512, (3, 3), activation='relu', padding='same',),\n",
    "    MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),\n",
    "    \n",
    "    # block 5\n",
    "    # 3 convolutions, each with:\n",
    "    #   512 3x3 kernels,\n",
    "    #   same padding (as described above),\n",
    "    #   relu activation\n",
    "    # and a maxpooling layer with strides of 2 and a window size of 2x2\n",
    "    Conv2D(512, (3, 3), activation='relu', padding='same',),\n",
    "    Conv2D(512, (3, 3), activation='relu', padding='same',),\n",
    "    Conv2D(512, (3, 3), activation='relu', padding='same',),\n",
    "    MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),\n",
    "    \n",
    "    # top \n",
    "    GlobalMaxPooling2D(),           # flatten into 1D for fully connected layers\n",
    "    Dense(512, activation='relu'),  # fully connected layer with activation relu \n",
    "    Dropout(0.5),                   # regularization to prevent overfitting\n",
    "    Dense(1, activation='sigmoid')  # final sigmoid for classification\n",
    "])\n",
    "\n",
    "# compile with model with binary cross entropy, as this is a binary classifcation problem,\n",
    "# and use stochastic gradient descent with a learning rate of 0.0001\n",
    "# and a momentum of 0.9 to help accelerate gradient in the right direction\n",
    "# and meausure fitness by accuracy\n",
    "model.compile(loss='binary_crossentropy', optimizer=SGD(lr=1e-4, momentum=0.9), metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, validate_df = train_test_split(df, test_size=0.20, random_state=42)  # split the training in train/test\n",
    "train_df = train_df.reset_index(drop=True)                                     # remove index column\n",
    "validate_df = validate_df.reset_index(drop=True)                               # remove index column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000\n",
      "5000\n"
     ]
    }
   ],
   "source": [
    "total_train = train_df.shape[0]        # get the total number of training images\n",
    "total_validate = validate_df.shape[0]  # get the total number of validation images\n",
    "\n",
    "print(total_train)                     # print number of training images\n",
    "print(total_validate)                  # print number of validation images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(  # generates images with augmentations specified below\n",
    "    rotation_range=15,               # rotate image 15 degrees\n",
    "    rescale=1./255,                  # target values between 0 and 1\n",
    "    shear_range=0.1,                 # counter-clockwise rotation\n",
    "    zoom_range=0.2,                  # zoom into image\n",
    "    horizontal_flip=True,            # allow horizontal flip\n",
    "    width_shift_range=0.1,           # shift width\n",
    "    height_shift_range=0.1           # shift height\n",
    ")\n",
    "\n",
    "# parameters\n",
    "flow_training_dir = training_dir+'/' # directory to flow images from\n",
    "img_dim = (img_wh, img_wh)           # set image dimensions without channels\n",
    "batch_size = 15                      # number of images to feed model at a time\n",
    "\n",
    "# create the image flow for training\n",
    "train_generator = train_datagen.flow_from_dataframe(\n",
    "    train_df,                        # dataframe of images and labels\n",
    "    flow_training_dir,               # directory to flow from\n",
    "    x_col='filename',                # specifying name of image identifier in dataframe\n",
    "    y_col='category',                # specifying name of label identifier in dataframe\n",
    "    target_size=img_dim,             # image dimensions without channels\n",
    "    class_mode='binary',             # binary classification: cat/dog\n",
    "    batch_size=batch_size            # number of images to process at a time\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "validation_datagen = ImageDataGenerator(rescale=1./255)  # target values between 0 and 1\n",
    "\n",
    "# create image flow for validation\n",
    "validation_generator = validation_datagen.flow_from_dataframe(\n",
    "    validate_df,                     # datagframe of images and labels\n",
    "    flow_training_dir,               # directory to flow from\n",
    "    x_col='filename',                # specifying name of image identifier in dataframe\n",
    "    y_col='category',                # specifying name of label identifier in dataframe\n",
    "    target_size=img_dim,             # image dimensions without channels\n",
    "    class_mode='binary',             # binary classification: cat/dog\n",
    "    batch_size=batch_size            # number of images to process at a time\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "1333/1333 [==============================] - 327s 245ms/step - loss: 0.7120 - acc: 0.6356 - val_loss: 0.5625 - val_acc: 0.7246\n",
      "Epoch 2/15\n",
      "1333/1333 [==============================] - 319s 240ms/step - loss: 0.5497 - acc: 0.7223 - val_loss: 0.7321 - val_acc: 0.6189\n",
      "Epoch 3/15\n",
      "1333/1333 [==============================] - 320s 240ms/step - loss: 0.4880 - acc: 0.7688 - val_loss: 0.5276 - val_acc: 0.7416\n",
      "Epoch 4/15\n",
      "1333/1333 [==============================] - 317s 238ms/step - loss: 0.4489 - acc: 0.7914 - val_loss: 0.3865 - val_acc: 0.8239\n",
      "Epoch 5/15\n",
      "1333/1333 [==============================] - 319s 240ms/step - loss: 0.4239 - acc: 0.8034 - val_loss: 0.3831 - val_acc: 0.8369\n",
      "Epoch 6/15\n",
      "1333/1333 [==============================] - 320s 240ms/step - loss: 0.3964 - acc: 0.8231 - val_loss: 0.4312 - val_acc: 0.8022\n",
      "Epoch 7/15\n",
      "1333/1333 [==============================] - 321s 241ms/step - loss: 0.3900 - acc: 0.8260 - val_loss: 0.7953 - val_acc: 0.7390\n",
      "Epoch 8/15\n",
      "1333/1333 [==============================] - 320s 240ms/step - loss: 0.3696 - acc: 0.8361 - val_loss: 0.3338 - val_acc: 0.8650\n",
      "Epoch 9/15\n",
      "1333/1333 [==============================] - 321s 241ms/step - loss: 0.3624 - acc: 0.8413 - val_loss: 0.4155 - val_acc: 0.8297\n",
      "Epoch 10/15\n",
      "1333/1333 [==============================] - 321s 241ms/step - loss: 0.3541 - acc: 0.8461 - val_loss: 0.3006 - val_acc: 0.8837\n",
      "Epoch 11/15\n",
      "1333/1333 [==============================] - 320s 240ms/step - loss: 0.3496 - acc: 0.8474 - val_loss: 0.3088 - val_acc: 0.8692\n",
      "Epoch 12/15\n",
      "1333/1333 [==============================] - 320s 240ms/step - loss: 0.3410 - acc: 0.8551 - val_loss: 0.2839 - val_acc: 0.8810\n",
      "Epoch 13/15\n",
      "1333/1333 [==============================] - 317s 238ms/step - loss: 0.3363 - acc: 0.8548 - val_loss: 0.2649 - val_acc: 0.8975\n",
      "Epoch 14/15\n",
      "1333/1333 [==============================] - 320s 240ms/step - loss: 0.3324 - acc: 0.8582 - val_loss: 0.3145 - val_acc: 0.8692\n",
      "Epoch 15/15\n",
      "1333/1333 [==============================] - 320s 240ms/step - loss: 0.3225 - acc: 0.8590 - val_loss: 0.2629 - val_acc: 0.8963\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f68705819e8>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_epochs = 15                                     # number of iterations\n",
    "model.fit_generator(                               # start fitting the data!\n",
    "    train_generator,                               # training image data generator\n",
    "    epochs=nb_epochs,                              # number of iterations\n",
    "    validation_data=validation_generator,          # validation image data generator\n",
    "    validation_steps=total_validate//batch_size,   # number of validation steps\n",
    "    steps_per_epoch=total_train//batch_size        # number of steps per epoch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
