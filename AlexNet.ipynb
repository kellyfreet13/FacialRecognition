{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# (1) Importing dependency\n",
    "from keras.models import Sequential\n",
    "import keras\n",
    "from keras.layers import Dense, Activation, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "#Implementing the mode weights\n",
    "model = Sequential()\n",
    "# Save the weights\n",
    "model.save_weights('model_weights.h5')\n",
    "\n",
    "# Save the model architecture\n",
    "with open('model_architecture.json', 'w') as f:\n",
    "    f.write(model.to_json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1st Convolutional Layer\n",
    "#model.add(Conv2D(filters=96, input_shape=(227,227,3), kernel_size=(11,11),\\\n",
    "# strides=(4,4), padding='valid'))\n",
    "#model.add(Activation('relu'))\n",
    "# Pooling \n",
    "#model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='valid'))\n",
    "# Batch Normalisation before passing it to the next layer\n",
    "#model.add(BatchNormalization())\n",
    "\n",
    "# 2nd Convolutional Layer\n",
    "#model.add(Conv2D(filters=256, kernel_size=(11,11), strides=(1,1), padding='valid'))\n",
    "#model.add(Activation('relu'))\n",
    "# Pooling\n",
    "#model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='valid'))\n",
    "# Batch Normalisation\n",
    "#model.add(BatchNormalization())\n",
    "\n",
    "# 3rd Convolutional Layer\n",
    "#model.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='valid'))\n",
    "#model.add(Activation('relu'))\n",
    "# Batch Normalisation\n",
    "#model.add(BatchNormalization())\n",
    "\n",
    "# 4th Convolutional Layer\n",
    "#model.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='valid'))\n",
    "#model.add(Activation('relu'))\n",
    "# Batch Normalisation\n",
    "#model.add(BatchNormalization())\n",
    "\n",
    "# 5th Convolutional Layer\n",
    "#model.add(Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), padding='valid'))\n",
    "#model.add(Activation('relu'))\n",
    "# Pooling\n",
    "#model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='valid'))\n",
    "# Batch Normalisation\n",
    "#model.add(BatchNormalization())\n",
    "\n",
    "# Passing it to a dense layer\n",
    "#model.add(Flatten())\n",
    "# 1st Dense Layer\n",
    "#model.add(Dense(4096, input_shape=(227*227*3,)))\n",
    "#model.add(Activation('relu'))\n",
    "# Add Dropout to prevent overfitting\n",
    "#model.add(Dropout(0.4))\n",
    "# Batch Normalisation\n",
    "#model.add(BatchNormalization())\n",
    "\n",
    "# 2nd Dense Layer\n",
    "#model.add(Dense(4096))\n",
    "#model.add(Activation('relu'))\n",
    "# Add Dropout\n",
    "#model.add(Dropout(0.4))\n",
    "# Batch Normalisation\n",
    "#model.add(BatchNormalization())\n",
    "\n",
    "# 3rd Dense Layer\n",
    "#model.add(Dense(1000))\n",
    "#model.add(Activation('relu'))\n",
    "# Add Dropout\n",
    "#model.add(Dropout(0.4))\n",
    "# Batch Normalisation\n",
    "#model.add(BatchNormalization())\n",
    "\n",
    "# Output Layer\n",
    "#model.add(Dense(3))\n",
    "#model.add(Activation('softmax'))\n",
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 55, 55, 96)        34944     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 55, 55, 96)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 27, 27, 96)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 27, 27, 96)        384       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 27, 27, 256)       614656    \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 27, 27, 256)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 13, 13, 256)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 13, 13, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 13, 13, 384)       885120    \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 13, 13, 384)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 13, 13, 384)       1536      \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 13, 13, 384)       1327488   \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 13, 13, 384)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 13, 13, 384)       1536      \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 13, 13, 256)       884992    \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 13, 13, 256)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 6, 6, 256)         1024      \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 4096)              37752832  \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 4096)              16384     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 4096)              16384     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 3)                 12291     \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 3)                 0         \n",
      "=================================================================\n",
      "Total params: 58,331,907\n",
      "Trainable params: 58,312,771\n",
      "Non-trainable params: 19,136\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 1st Convolutional Layer\n",
    "model.add(Conv2D(filters=96, input_shape=(227,227,3), kernel_size=(11,11),\\\n",
    " strides=(4,4), padding='valid'))\n",
    "model.add(Activation('relu'))\n",
    "# Pooling \n",
    "model.add(MaxPooling2D(pool_size=(3,3), strides=(2,2), padding='valid'))\n",
    "# Batch Normalisation before passing it to the next layer\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# 2nd Convolutional Layer\n",
    "model.add(Conv2D(filters=256, kernel_size=(5,5), strides=(1,1), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "# Pooling\n",
    "model.add(MaxPooling2D(pool_size=(3,3), strides=(2,2), padding='valid'))\n",
    "# Batch Normalisation\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# 3rd Convolutional Layer\n",
    "model.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "# Batch Normalisation\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# 4th Convolutional Layer\n",
    "model.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "# Batch Normalisation\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# 5th Convolutional Layer\n",
    "model.add(Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "# Pooling\n",
    "model.add(MaxPooling2D(pool_size=(3,3), strides=(2,2), padding='valid'))\n",
    "# Batch Normalisation\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# Passing it to a dense layer\n",
    "model.add(Flatten())\n",
    "# 1st Dense Layer\n",
    "model.add(Dense(4096, input_shape=(227*227*3,)))\n",
    "model.add(Activation('relu'))\n",
    "# Add Dropout to prevent overfitting\n",
    "model.add(Dropout(0.4))\n",
    "# Batch Normalisation\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# 2nd Dense Layer\n",
    "model.add(Dense(4096))\n",
    "model.add(Activation('relu'))\n",
    "# Add Dropout\n",
    "model.add(Dropout(0.4))\n",
    "# Batch Normalisation\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# 3rd Dense Layer\n",
    "#model.add(Dense(1000))\n",
    "#model.add(Activation('relu'))\n",
    "# Add Dropout\n",
    "#model.add(Dropout(0.4))\n",
    "# Batch Normalisation\n",
    "#model.add(BatchNormalization())\n",
    "\n",
    "# Output Layer\n",
    "model.add(Dense(3))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import keras\n",
    "\n",
    "#target_csv = '/train_target.csv'\n",
    "\n",
    "#target_df = pd.read_csv(train_dir+target_csv, header=None)\n",
    "\n",
    "#train_labels = np.zeros((nb_train_sample, 1))\n",
    "#test_labels = np.zeros((nb_test_sample, 1))\n",
    "\n",
    "#for i, row in target_df.iterrows():\n",
    "#    if i < nb_train_sample:\n",
    "#        train_labels[i] = row\n",
    "#    else:\n",
    "#        test_labels[i-nb_train_sample] = row\n",
    "        \n",
    "#train_labels = keras.utils.to_categorical(train_labels, num_classes=3)\n",
    "#test_labels = keras.utils.to_categorical(test_labels, num_classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directories\n",
    "train_dir = './input/training/'\n",
    "train_folders = os.listdir(train_dir)\n",
    "test_dir = './input/testing/'\n",
    "test_folders = os.listdir(test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading images from ./input/training/2/ ...\n",
      "Reading images from ./input/training/0/ ...\n",
      "Reading images from ./input/training/1/ ...\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "x_train, y_train = [], []\n",
    "for folder in train_folders:\n",
    "    files = os.listdir(train_dir + folder)\n",
    "    print('Reading images from ' + train_dir + folder + '/ ...')\n",
    "    for file in files:\n",
    "        img = cv2.imread(train_dir + folder + '/' + file)\n",
    "        img = cv2.resize(img, (227, 227))\n",
    "        img = img.reshape(-1, 227, 227, 3)\n",
    "        x_train.append(img)\n",
    "        y_train.append(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading image from ./input/testing/2/ ...\n",
      "Reading image from ./input/testing/0/ ...\n",
      "Reading image from ./input/testing/1/ ...\n"
     ]
    }
   ],
   "source": [
    "x_test, y_test = [], []\n",
    "for folder in test_folders:\n",
    "    files = os.listdir(test_dir + folder)\n",
    "    print('Reading image from ' + test_dir + folder + '/ ...')\n",
    "    for file in files:\n",
    "        img = cv2.imread(test_dir + folder + '/' + file)\n",
    "        img = cv2.resize(img, (227,227))\n",
    "        x_test.append(img)\n",
    "        y_test.append(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 14000 images belonging to 3 classes.\n",
      "Found 2174 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "# image preprocessing\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1./255,\n",
    "                                   shear_range=0.2,\n",
    "                                   zoom_range=0.2,\n",
    "                                   width_shift_range=0.2,\n",
    "                                   height_shift_range=0.2,\n",
    "                                   fill_mode='nearest')\n",
    "\n",
    "valid_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "training_set = train_datagen.flow_from_directory(train_dir,\n",
    "                                                 target_size=(227, 227),\n",
    "                                                 batch_size=batch_size,\n",
    "                                                 class_mode='categorical')\n",
    "\n",
    "valid_set = valid_datagen.flow_from_directory(test_dir,\n",
    "                                            target_size=(227, 227),\n",
    "                                            batch_size=batch_size,\n",
    "                                            class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the Model\n",
    "from keras import optimizers\n",
    "model.compile(optimizer=optimizers.SGD(lr=0.01, momentum=0.09, decay=1e-6, nesterov=True),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_num = training_set.samples\n",
    "valid_num = valid_set.samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/100\n",
      "875/875 [==============================] - 206s 235ms/step - loss: 1.9814 - acc: 0.3717 - val_loss: 2.0201 - val_acc: 0.3574\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.35741, saving model to best_weights_9.hdf5\n",
      "Epoch 2/100\n",
      "875/875 [==============================] - 200s 229ms/step - loss: 1.9645 - acc: 0.3788 - val_loss: 2.0211 - val_acc: 0.3670\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.35741 to 0.36701, saving model to best_weights_9.hdf5\n",
      "Epoch 3/100\n",
      "875/875 [==============================] - 201s 229ms/step - loss: 1.9876 - acc: 0.3784 - val_loss: 1.4476 - val_acc: 0.4249\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.36701 to 0.42493, saving model to best_weights_9.hdf5\n",
      "Epoch 4/100\n",
      "875/875 [==============================] - 202s 231ms/step - loss: 1.9825 - acc: 0.3898 - val_loss: 1.5743 - val_acc: 0.4291\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.42493 to 0.42910, saving model to best_weights_9.hdf5\n",
      "Epoch 5/100\n",
      "875/875 [==============================] - 199s 227ms/step - loss: 1.9330 - acc: 0.4041 - val_loss: 1.9466 - val_acc: 0.4657\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.42910 to 0.46571, saving model to best_weights_9.hdf5\n",
      "Epoch 6/100\n",
      "875/875 [==============================] - 200s 229ms/step - loss: 1.8852 - acc: 0.4074 - val_loss: 1.4113 - val_acc: 0.4601\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.46571\n",
      "Epoch 7/100\n",
      "875/875 [==============================] - 201s 230ms/step - loss: 1.8240 - acc: 0.4169 - val_loss: 1.6261 - val_acc: 0.4884\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.46571 to 0.48842, saving model to best_weights_9.hdf5\n",
      "Epoch 8/100\n",
      "875/875 [==============================] - 201s 230ms/step - loss: 1.7951 - acc: 0.4328 - val_loss: 4.5704 - val_acc: 0.3318\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.48842\n",
      "Epoch 9/100\n",
      "875/875 [==============================] - 200s 229ms/step - loss: 1.7335 - acc: 0.4444 - val_loss: 1.3640 - val_acc: 0.5162\n",
      "\n",
      "Epoch 00009: val_acc improved from 0.48842 to 0.51622, saving model to best_weights_9.hdf5\n",
      "Epoch 10/100\n",
      "875/875 [==============================] - 201s 229ms/step - loss: 1.6796 - acc: 0.4600 - val_loss: 1.1897 - val_acc: 0.5343\n",
      "\n",
      "Epoch 00010: val_acc improved from 0.51622 to 0.53429, saving model to best_weights_9.hdf5\n",
      "Epoch 11/100\n",
      "875/875 [==============================] - 202s 231ms/step - loss: 1.6461 - acc: 0.4644 - val_loss: 1.5710 - val_acc: 0.4305\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.53429\n",
      "Epoch 12/100\n",
      "875/875 [==============================] - 201s 230ms/step - loss: 1.5720 - acc: 0.4890 - val_loss: 1.7018 - val_acc: 0.5375\n",
      "\n",
      "Epoch 00012: val_acc improved from 0.53429 to 0.53753, saving model to best_weights_9.hdf5\n",
      "Epoch 13/100\n",
      "875/875 [==============================] - 200s 229ms/step - loss: 1.5238 - acc: 0.5035 - val_loss: 1.5359 - val_acc: 0.5181\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.53753\n",
      "Epoch 14/100\n",
      "875/875 [==============================] - 203s 231ms/step - loss: 1.4650 - acc: 0.5217 - val_loss: 1.2776 - val_acc: 0.5561\n",
      "\n",
      "Epoch 00014: val_acc improved from 0.53753 to 0.55607, saving model to best_weights_9.hdf5\n",
      "Epoch 15/100\n",
      "875/875 [==============================] - 201s 229ms/step - loss: 1.4138 - acc: 0.5316 - val_loss: 0.9768 - val_acc: 0.6302\n",
      "\n",
      "Epoch 00015: val_acc improved from 0.55607 to 0.63021, saving model to best_weights_9.hdf5\n",
      "Epoch 16/100\n",
      "875/875 [==============================] - 200s 229ms/step - loss: 1.3646 - acc: 0.5486 - val_loss: 1.2376 - val_acc: 0.5857\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.63021\n",
      "Epoch 17/100\n",
      "875/875 [==============================] - 201s 230ms/step - loss: 1.2805 - acc: 0.5676 - val_loss: 7.3304 - val_acc: 0.3244\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.63021\n",
      "Epoch 18/100\n",
      "875/875 [==============================] - 201s 230ms/step - loss: 1.2560 - acc: 0.5744 - val_loss: 0.8409 - val_acc: 0.6742\n",
      "\n",
      "Epoch 00018: val_acc improved from 0.63021 to 0.67424, saving model to best_weights_9.hdf5\n",
      "Epoch 19/100\n",
      "875/875 [==============================] - 200s 229ms/step - loss: 1.1991 - acc: 0.5835 - val_loss: 0.7906 - val_acc: 0.6766\n",
      "\n",
      "Epoch 00019: val_acc improved from 0.67424 to 0.67655, saving model to best_weights_9.hdf5\n",
      "Epoch 20/100\n",
      "875/875 [==============================] - 200s 228ms/step - loss: 1.1666 - acc: 0.6014 - val_loss: 0.8122 - val_acc: 0.6918\n",
      "\n",
      "Epoch 00020: val_acc improved from 0.67655 to 0.69184, saving model to best_weights_9.hdf5\n",
      "Epoch 21/100\n",
      "875/875 [==============================] - 202s 230ms/step - loss: 1.1103 - acc: 0.6179 - val_loss: 0.8593 - val_acc: 0.6844\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.69184\n",
      "Epoch 22/100\n",
      "875/875 [==============================] - 201s 230ms/step - loss: 1.0635 - acc: 0.6276 - val_loss: 1.0419 - val_acc: 0.5677\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.69184\n",
      "Epoch 23/100\n",
      "875/875 [==============================] - 202s 231ms/step - loss: 1.0363 - acc: 0.6351 - val_loss: 1.0199 - val_acc: 0.6098\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.69184\n",
      "Epoch 24/100\n",
      "875/875 [==============================] - 202s 230ms/step - loss: 1.0122 - acc: 0.6441 - val_loss: 1.3433 - val_acc: 0.5704\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.69184\n",
      "Epoch 25/100\n",
      "875/875 [==============================] - 203s 232ms/step - loss: 0.9730 - acc: 0.6516 - val_loss: 1.0013 - val_acc: 0.6784\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.69184\n",
      "Epoch 26/100\n",
      "875/875 [==============================] - 202s 230ms/step - loss: 0.9486 - acc: 0.6629 - val_loss: 0.7764 - val_acc: 0.7118\n",
      "\n",
      "Epoch 00026: val_acc improved from 0.69184 to 0.71177, saving model to best_weights_9.hdf5\n",
      "Epoch 27/100\n",
      "875/875 [==============================] - 201s 230ms/step - loss: 0.9373 - acc: 0.6613 - val_loss: 1.0167 - val_acc: 0.6752\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.71177\n",
      "Epoch 28/100\n",
      "875/875 [==============================] - 203s 232ms/step - loss: 0.9044 - acc: 0.6754 - val_loss: 1.6572 - val_acc: 0.6321\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.71177\n",
      "Epoch 29/100\n",
      "875/875 [==============================] - 202s 231ms/step - loss: 0.8698 - acc: 0.6795 - val_loss: 0.7204 - val_acc: 0.7280\n",
      "\n",
      "Epoch 00029: val_acc improved from 0.71177 to 0.72799, saving model to best_weights_9.hdf5\n",
      "Epoch 30/100\n",
      "875/875 [==============================] - 201s 230ms/step - loss: 0.8780 - acc: 0.6851 - val_loss: 0.8047 - val_acc: 0.6835\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.72799\n",
      "Epoch 31/100\n",
      "875/875 [==============================] - 201s 229ms/step - loss: 0.8404 - acc: 0.6866 - val_loss: 0.6140 - val_acc: 0.7665\n",
      "\n",
      "Epoch 00031: val_acc improved from 0.72799 to 0.76645, saving model to best_weights_9.hdf5\n",
      "Epoch 32/100\n",
      "875/875 [==============================] - 200s 229ms/step - loss: 0.8348 - acc: 0.6941 - val_loss: 0.7202 - val_acc: 0.7470\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.76645\n",
      "Epoch 33/100\n",
      "875/875 [==============================] - 200s 229ms/step - loss: 0.8041 - acc: 0.7084 - val_loss: 0.6360 - val_acc: 0.7340\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.76645\n",
      "Epoch 34/100\n",
      "875/875 [==============================] - 203s 232ms/step - loss: 0.8110 - acc: 0.7016 - val_loss: 0.8495 - val_acc: 0.6506\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 0.76645\n",
      "Epoch 35/100\n",
      "875/875 [==============================] - 203s 232ms/step - loss: 0.7933 - acc: 0.7052 - val_loss: 0.6771 - val_acc: 0.7563\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.76645\n",
      "Epoch 36/100\n",
      "875/875 [==============================] - 202s 231ms/step - loss: 0.7770 - acc: 0.7125 - val_loss: 0.5867 - val_acc: 0.7864\n",
      "\n",
      "Epoch 00036: val_acc improved from 0.76645 to 0.78638, saving model to best_weights_9.hdf5\n",
      "Epoch 37/100\n",
      "875/875 [==============================] - 203s 232ms/step - loss: 0.7574 - acc: 0.7204 - val_loss: 0.6443 - val_acc: 0.7247\n",
      "\n",
      "Epoch 00037: val_acc did not improve from 0.78638\n",
      "Epoch 38/100\n",
      "875/875 [==============================] - 205s 235ms/step - loss: 0.7344 - acc: 0.7263 - val_loss: 0.5262 - val_acc: 0.8100\n",
      "\n",
      "Epoch 00038: val_acc improved from 0.78638 to 0.81001, saving model to best_weights_9.hdf5\n",
      "Epoch 39/100\n",
      "875/875 [==============================] - 203s 231ms/step - loss: 0.7328 - acc: 0.7290 - val_loss: 0.6547 - val_acc: 0.7817\n",
      "\n",
      "Epoch 00039: val_acc did not improve from 0.81001\n",
      "Epoch 40/100\n",
      "875/875 [==============================] - 203s 232ms/step - loss: 0.7109 - acc: 0.7370 - val_loss: 0.7099 - val_acc: 0.7298\n",
      "\n",
      "Epoch 00040: val_acc did not improve from 0.81001\n",
      "Epoch 41/100\n",
      "875/875 [==============================] - 203s 233ms/step - loss: 0.7077 - acc: 0.7347 - val_loss: 0.7172 - val_acc: 0.7405\n",
      "\n",
      "Epoch 00041: val_acc did not improve from 0.81001\n",
      "Epoch 42/100\n",
      "875/875 [==============================] - 201s 230ms/step - loss: 0.7067 - acc: 0.7379 - val_loss: 0.6099 - val_acc: 0.7836\n",
      "\n",
      "Epoch 00042: val_acc did not improve from 0.81001\n",
      "Epoch 43/100\n",
      "875/875 [==============================] - 201s 230ms/step - loss: 0.6889 - acc: 0.7422 - val_loss: 0.6697 - val_acc: 0.7535\n",
      "\n",
      "Epoch 00043: val_acc did not improve from 0.81001\n",
      "Epoch 44/100\n",
      "875/875 [==============================] - 202s 230ms/step - loss: 0.6595 - acc: 0.7481 - val_loss: 0.6611 - val_acc: 0.7201\n",
      "\n",
      "Epoch 00044: val_acc did not improve from 0.81001\n",
      "Epoch 45/100\n",
      "875/875 [==============================] - 202s 230ms/step - loss: 0.6664 - acc: 0.7492 - val_loss: 0.6510 - val_acc: 0.7794\n",
      "\n",
      "Epoch 00045: val_acc did not improve from 0.81001\n",
      "Epoch 46/100\n",
      "875/875 [==============================] - 202s 231ms/step - loss: 0.6573 - acc: 0.7552 - val_loss: 0.9808 - val_acc: 0.7020\n",
      "\n",
      "Epoch 00046: val_acc did not improve from 0.81001\n",
      "Epoch 47/100\n",
      "875/875 [==============================] - 201s 229ms/step - loss: 0.6363 - acc: 0.7621 - val_loss: 0.6008 - val_acc: 0.7905\n",
      "\n",
      "Epoch 00047: val_acc did not improve from 0.81001\n",
      "Epoch 48/100\n",
      "875/875 [==============================] - 201s 229ms/step - loss: 0.6384 - acc: 0.7649 - val_loss: 0.5982 - val_acc: 0.7771\n",
      "\n",
      "Epoch 00048: val_acc did not improve from 0.81001\n",
      "Epoch 49/100\n",
      "875/875 [==============================] - 201s 230ms/step - loss: 0.6323 - acc: 0.7684 - val_loss: 0.5488 - val_acc: 0.7970\n",
      "\n",
      "Epoch 00049: val_acc did not improve from 0.81001\n",
      "Epoch 50/100\n",
      "875/875 [==============================] - 203s 231ms/step - loss: 0.6085 - acc: 0.7764 - val_loss: 1.1800 - val_acc: 0.6080\n",
      "\n",
      "Epoch 00050: val_acc did not improve from 0.81001\n",
      "Epoch 51/100\n",
      "875/875 [==============================] - 199s 228ms/step - loss: 0.5983 - acc: 0.7790 - val_loss: 0.6767 - val_acc: 0.7632\n",
      "\n",
      "Epoch 00051: val_acc did not improve from 0.81001\n",
      "Epoch 52/100\n",
      "875/875 [==============================] - 202s 231ms/step - loss: 0.5846 - acc: 0.7797 - val_loss: 0.6492 - val_acc: 0.7804\n",
      "\n",
      "Epoch 00052: val_acc did not improve from 0.81001\n",
      "Epoch 53/100\n",
      "875/875 [==============================] - 202s 231ms/step - loss: 0.5922 - acc: 0.7798 - val_loss: 0.4890 - val_acc: 0.8105\n",
      "\n",
      "Epoch 00053: val_acc improved from 0.81001 to 0.81047, saving model to best_weights_9.hdf5\n",
      "Epoch 54/100\n",
      "875/875 [==============================] - 202s 231ms/step - loss: 0.5734 - acc: 0.7849 - val_loss: 0.5295 - val_acc: 0.7896\n",
      "\n",
      "Epoch 00054: val_acc did not improve from 0.81047\n",
      "Epoch 55/100\n",
      "875/875 [==============================] - 201s 229ms/step - loss: 0.5686 - acc: 0.7887 - val_loss: 0.5833 - val_acc: 0.7813\n",
      "\n",
      "Epoch 00055: val_acc did not improve from 0.81047\n",
      "Epoch 56/100\n",
      "875/875 [==============================] - 203s 232ms/step - loss: 0.5576 - acc: 0.7906 - val_loss: 0.5245 - val_acc: 0.8091\n",
      "\n",
      "Epoch 00056: val_acc did not improve from 0.81047\n",
      "Epoch 57/100\n",
      "875/875 [==============================] - 202s 231ms/step - loss: 0.5469 - acc: 0.7964 - val_loss: 0.6880 - val_acc: 0.7289\n",
      "\n",
      "Epoch 00057: val_acc did not improve from 0.81047\n",
      "Epoch 58/100\n",
      "875/875 [==============================] - 200s 228ms/step - loss: 0.5273 - acc: 0.8019 - val_loss: 0.8144 - val_acc: 0.7516\n",
      "\n",
      "Epoch 00058: val_acc did not improve from 0.81047\n",
      "Epoch 59/100\n",
      "875/875 [==============================] - 202s 230ms/step - loss: 0.5272 - acc: 0.8039 - val_loss: 0.7122 - val_acc: 0.7609\n",
      "\n",
      "Epoch 00059: val_acc did not improve from 0.81047\n",
      "Epoch 60/100\n",
      "875/875 [==============================] - 203s 233ms/step - loss: 0.5203 - acc: 0.8054 - val_loss: 0.5991 - val_acc: 0.7813\n",
      "\n",
      "Epoch 00060: val_acc did not improve from 0.81047\n",
      "Epoch 61/100\n",
      "875/875 [==============================] - 200s 228ms/step - loss: 0.5069 - acc: 0.8107 - val_loss: 0.5495 - val_acc: 0.8165\n",
      "\n",
      "Epoch 00061: val_acc improved from 0.81047 to 0.81650, saving model to best_weights_9.hdf5\n",
      "Epoch 62/100\n",
      "875/875 [==============================] - 202s 231ms/step - loss: 0.4931 - acc: 0.8190 - val_loss: 0.5962 - val_acc: 0.7813\n",
      "\n",
      "Epoch 00062: val_acc did not improve from 0.81650\n",
      "Epoch 63/100\n",
      "875/875 [==============================] - 200s 229ms/step - loss: 0.4850 - acc: 0.8225 - val_loss: 0.6314 - val_acc: 0.7669\n",
      "\n",
      "Epoch 00063: val_acc did not improve from 0.81650\n",
      "Epoch 64/100\n",
      "553/875 [=================>............] - ETA: 1:11 - loss: 0.4614 - acc: 0.8288"
     ]
    }
   ],
   "source": [
    "# checkpoint\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "weightpath = \"best_weights_9.hdf5\"\n",
    "checkpoint = ModelCheckpoint(weightpath, monitor='val_acc', verbose=1, save_best_only=True, save_weights_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "#fitting images to CNN\n",
    "history = model.fit_generator(training_set,\n",
    "                         steps_per_epoch=train_num//batch_size,\n",
    "                         validation_data=valid_set,\n",
    "                         epochs=100,\n",
    "                         validation_steps=valid_num//batch_size,\n",
    "                         callbacks=callbacks_list)\n",
    "#saving model\n",
    "filepath=\"AlexNetModel.hdf5\"\n",
    "model.save(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def plot_model_history(model_history, acc='acc', val_acc='val_acc'):\n",
    "    fig, axs = plt.subplots(1,2,figsize=(15,5))\n",
    "    axs[0].plot(range(1,len(model_history.history[acc])+1),model_history.history[acc])\n",
    "    axs[0].plot(range(1,len(model_history.history[val_acc])+1),model_history.history[val_acc])\n",
    "    axs[0].set_title('Model Accuracy')\n",
    "    axs[0].set_ylabel('Accuracy')\n",
    "    axs[0].set_xlabel('Epoch')\n",
    "    axs[0].set_xticks(np.arange(1,len(model_history.history[acc])+1),len(model_history.history[acc])/10)\n",
    "    axs[0].legend(['train', 'val'], loc='best')\n",
    "    axs[1].plot(range(1,len(model_history.history['loss'])+1),model_history.history['loss'])\n",
    "    axs[1].plot(range(1,len(model_history.history['val_loss'])+1),model_history.history['val_loss'])\n",
    "    axs[1].set_title('Model Loss')\n",
    "    axs[1].set_ylabel('Loss')\n",
    "    axs[1].set_xlabel('Epoch')\n",
    "    axs[1].set_xticks(np.arange(1,len(model_history.history['loss'])+1),len(model_history.history['loss'])/10)\n",
    "    axs[1].legend(['train', 'val'], loc='best')\n",
    "    plt.show()\n",
    "\n",
    "plot_model_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
